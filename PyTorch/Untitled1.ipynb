{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a73e01-2947-4abc-96a9-166a1a4e0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ff553b-5cdd-4231-a74d-9ad4059ae1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of the whole_seq_output :  torch.Size([10, 32, 4])\n",
      "Output shape of the h_T :  torch.Size([1, 32, 4])\n",
      "Output shape of the c_T :  torch.Size([1, 32, 4])\n",
      "\n",
      "Verifying the last element (in timesteps axis) of whole_seq_output is same as h_T.\n",
      "Is whole_seq_output[-1, :, :] == h_T ?  True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "timesteps = 10\n",
    "input_features = 8\n",
    "output_features = 4\n",
    "\n",
    "inputs = torch.randn(timesteps, batch_size, input_features)\n",
    "lstm = torch.nn.LSTM(input_size=input_features,hidden_size=output_features)\n",
    "whole_seq_output, (h_T, c_T) = lstm(inputs)\n",
    "\n",
    "is_equal =  torch.all(torch.eq(whole_seq_output[-1,:,:], h_T))\n",
    "\n",
    "print(\"Output shape of the whole_seq_output : \", whole_seq_output.shape)\n",
    "print(\"Output shape of the h_T : \", h_T.shape)\n",
    "print(\"Output shape of the c_T : \", c_T.shape)\n",
    "\n",
    "print(\"\\nVerifying the last element (in timesteps axis) of whole_seq_output is same as h_T.\")\n",
    "print(\"Is whole_seq_output[-1, :, :] == h_T ? \", is_equal.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9332f8e-646d-4584-b494-0e62ae603209",
   "metadata": {},
   "source": [
    "* \"1.   `whole_seq_output` is of the shape :    `(timesteps, batch_size, output_features)` - $(T,n_{batch},n_h)$ \",\n",
    "        \"This corresponds to $h^t : \\\\forall t = 1,2...T$.\\n\",\n",
    "* \"2.   `h_T` is of the shape :    `(1, batch_size, output_features)` - $(1, n_{batch},n_h)$  \\n\",\n",
    "        \"This corresponds to $h^T$, where $T$ is the last `timestep` in the sequence.\\n\",\n",
    "* \"3. `c_T` is of the shape :    `(1, batch_size, output_features)` - $(1, n_{batch},n_h)$  \\n\",\n",
    "        \"This corresponds to $c^T$, where $T$ is the last `timestep` in the sequence.\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8aeadd-3fa3-47ef-85b7-fe4c6a827ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleModel                              [32, 1]                   --\n",
       "├─LSTM: 1-1                              [12, 32, 8]               832\n",
       "├─LSTM: 1-2                              [12, 32, 4]               224\n",
       "├─Linear: 1-3                            [32, 2]                   10\n",
       "├─ReLU: 1-4                              [32, 2]                   --\n",
       "├─Linear: 1-5                            [32, 1]                   3\n",
       "├─Sigmoid: 1-6                           [32, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 1,069\n",
       "Trainable params: 1,069\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.41\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "timesteps = 12\n",
    "input_features = 16\n",
    "h1_features = 8\n",
    "h2_features = 4\n",
    "h3_features = 2\n",
    "output_features = 1\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.lstm1 = torch.nn.LSTM(input_size=input_features, hidden_size=h1_features)\n",
    "    self.lstm2 = torch.nn.LSTM(input_size=h1_features, hidden_size=h2_features)\n",
    "    self.fc1 = torch.nn.Linear(h2_features, h3_features)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.fc2 = torch.nn.Linear(h3_features, output_features)\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    h1, (h1_T,c1_T) = self.lstm1(inputs)\n",
    "    h2, (h2_T, c2_T) = self.lstm2(h1)\n",
    "    h3 = self.fc1(h2[-1,:,:])       # inplace of h2[-1,:,:] we can use h2_T. Both are identical\n",
    "    h3 = self.relu(h3)\n",
    "    output = self.fc2(h3)\n",
    "    output = self.sigmoid(output)\n",
    "    return output\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "torchinfo.summary(model,(timesteps, batch_size, input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03950512-61be-4c65-9ca4-6620410b177a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
